{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'ssd_inception_v2_coco_2017_11_17'\n",
    "# high accuracy but very slow\n",
    "\n",
    "# MODEL_NAME = 'faster_rcnn_resnet101_coco_2017_11_08'\n",
    "\n",
    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = os.path.join('object_detection/data', 'mscoco_label_map.pbtxt')\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "opener = urllib.request.URLopener()\n",
    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "tar_file = tarfile.open(MODEL_FILE)\n",
    "for file in tar_file.getmembers():\n",
    "    file_name = os.path.basename(file.name)\n",
    "    if 'frozen_inference_graph.pb' in file_name:\n",
    "        tar_file.extract(file, os.getcwd())\n",
    "    \n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "    \n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "sess = tf.Session(graph=detection_graph)\n",
    "# Define input and output tensors (i.e. data) for the object detection classifier\n",
    "\n",
    "# Input tensor is the image\n",
    "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "# Output tensors are the detection boxes, scores, and classes\n",
    "# Each box represents a part of the image where a particular object was detected\n",
    "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "# Each score represents level of confidence for each of the objects.\n",
    "# The score is shown on the result image, together with the class label.\n",
    "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "\n",
    "# Number of objects detected\n",
    "num_detections = detection_graph.get_tensor_by_name('num_detections:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_of_interest(img, vertices):\n",
    "    mask = np.zeros_like(img)   \n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255   \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyttsx3\n",
    "# import pywintypes\n",
    "# import pythoncom\n",
    "# import win32api\n",
    "# import win32api\n",
    "# engine = pyttsx3.init()\n",
    "# engine.say('Hi')\n",
    "# engine.runAndWait()\n",
    "\n",
    "# import win32com.client\n",
    "\n",
    "# import os\n",
    "# os.system(\"espeak 'Hi'\")\n",
    "\n",
    "# !pip install pywin32 pypiwin32 pyttsx3\n",
    "# import os\n",
    "# import sys\n",
    "import pyttsx3\n",
    "\n",
    "en_voice_id = \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0\"\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "# engine.setProperty('rate', 100)\n",
    "# engine.setProperty('voice', en_voice_id)\n",
    "\n",
    "# voices = engine.getProperty('voices')\n",
    "# for voice in voices:\n",
    "#     print(\"Voice:\")\n",
    "#     print(\" - ID: %s\" % voice.id)\n",
    "#     print(\" - Name: %s\" % voice.name)\n",
    "#     print(\" - Languages: %s\" % voice.languages)\n",
    "#     print(\" - Gender: %s\" % voice.gender)\n",
    "#     print(\" - Age: %s\" % voice.age)\n",
    "\n",
    "#from gtts import gTTS\n",
    "# from espeakng import ESpeakNG\n",
    "# esng = ESpeakNG()\n",
    "# esng.say('Go on')\n",
    "\n",
    "# from espeak import espeak\n",
    "# espeak.synth('Hi')\n",
    "\n",
    "# from tts_watson.TtsWatson import TtsWatson\n",
    "# ttsWatson = TtsWatson('watson_user', 'watson_password', 'en-US_AllisonVoice') \n",
    "# ttsWatson.play(\"Hello World\")\n",
    "\n",
    "# import talkey\n",
    "\n",
    "# tts = talkey.Talkey()\n",
    "# tts.say('Hi there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = cv2.VideoCapture('test_video_1.mp4')\n",
    "try:\n",
    "    def detection():\n",
    "        print(\"dscs\")\n",
    "        url=\"http://192.168.137.166:8080/shot.jpg\"\n",
    "\n",
    "        # video = cv2.VideoCapture('test_video_1.mp4')\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        imgresp=urllib.request.urlopen(url)\n",
    "        imgno=np.array(bytearray(imgresp.read()),dtype=np.uint8)\n",
    "\n",
    "        frame=cv2.imdecode(imgno,-1)\n",
    "        frame=cv2.transpose(frame)\n",
    "        frame=cv2.flip(frame,flipCode=1)\n",
    "        print(frame.shape)\n",
    "        frame_width = frame.shape[0]\n",
    "        frame_height = frame.shape[1]\n",
    "\n",
    "        # frame_width = int(video.get(3))\n",
    "        # frame_height = int(video.get(4))\n",
    "\n",
    "        #out = cv2.VideoWriter('output.avi',fourcc, 20.0, (frame_width,frame_height))\n",
    "        # out = cv2.VideoWriter('outpy.mp4',cv2.VideoWriter_fourcc('M','J','P','G'), 20, (frame_width,frame_height))\n",
    "        #ut = cv2.VideoWriter('output_video.avi',cv2.VideoWriter_fourcc(*'MJPG',20,(1280,960)))\n",
    "        while(True):\n",
    "            imgresp=urllib.request.urlopen(url)\n",
    "            imgno=np.array(bytearray(imgresp.read()),dtype=np.uint8)\n",
    "\n",
    "            frame=cv2.imdecode(imgno,-1)\n",
    "            frame=cv2.transpose(frame)\n",
    "            frame=cv2.flip(frame,flipCode=1)\n",
    "            frame = cv2.resize(frame, (740,600))\n",
    "            #print(frame.shape)\n",
    "            #ret, frame = video.read()\n",
    "            stime = time.time()\n",
    "            objects = []\n",
    "            class_str = \"\"\n",
    "            frame_width = frame.shape[0]\n",
    "            frame_height = frame.shape[1]\n",
    "            rows, cols = frame.shape[:2]\n",
    "            left_boundary = [int(cols*0.40), int(rows*0.95)]\n",
    "            left_boundary_top = [int(cols*0.40), int(rows*0.20)]\n",
    "            right_boundary = [int(cols*0.60), int(rows*0.95)]\n",
    "            right_boundary_top = [int(cols*0.60), int(rows*0.20)]\n",
    "            bottom_left  = [int(cols*0.20), int(rows*0.95)]\n",
    "            top_left     = [int(cols*0.20), int(rows*0.20)]\n",
    "            bottom_right = [int(cols*0.80), int(rows*0.95)]\n",
    "            top_right    = [int(cols*0.80), int(rows*0.20)]\n",
    "            vertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)\n",
    "\n",
    "            cv2.line(frame,tuple(bottom_left),tuple(bottom_right), (255, 0, 0), 5)\n",
    "            cv2.line(frame,tuple(bottom_right),tuple(top_right), (255, 0, 0), 5)\n",
    "            cv2.line(frame,tuple(top_left),tuple(bottom_left), (255, 0, 0), 5)\n",
    "            cv2.line(frame,tuple(top_left),tuple(top_right), (255, 0, 0), 5)\n",
    "            copied = np.copy(frame)\n",
    "            interested = region_of_interest(copied,vertices)\n",
    "            frame_expanded = np.expand_dims(interested, axis=0)\n",
    "\n",
    "            (boxes, scores, classes, num) = sess.run([detection_boxes, detection_scores, detection_classes, num_detections],feed_dict={image_tensor: frame_expanded})\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                frame,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=8,\n",
    "                min_score_thresh=0.78)\n",
    "            print(frame_width,frame_height)\n",
    "            ymin = int((boxes[0][0][0]*frame_width))\n",
    "            xmin = int((boxes[0][0][1]*frame_height))\n",
    "            ymax = int((boxes[0][0][2]*frame_width))\n",
    "            xmax = int((boxes[0][0][3]*frame_height))\n",
    "            Result = np.array(frame[ymin:ymax,xmin:xmax])\n",
    "\n",
    "            ymin_str='y min  = %.2f '%(ymin)\n",
    "            ymax_str='y max  = %.2f '%(ymax)\n",
    "            xmin_str='x min  = %.2f '%(xmin)\n",
    "            xmax_str='x max  = %.2f '%(xmax)\n",
    "\n",
    "            cv2.putText(frame,ymin_str, (50, 50),cv2.FONT_HERSHEY_SIMPLEX,0.6,(255,0,0),2)\n",
    "            cv2.putText(frame,ymax_str, (50, 70),cv2.FONT_HERSHEY_SIMPLEX,0.6,(255,0,0),2)\n",
    "            cv2.putText(frame,xmin_str, (50, 90),cv2.FONT_HERSHEY_SIMPLEX,0.6,(255,0,0),2)\n",
    "            cv2.putText(frame,xmax_str, (50, 110),cv2.FONT_HERSHEY_SIMPLEX,0.6,(255,0,0),2)\n",
    "            print(scores.max())\n",
    "\n",
    "            print(\"left_boundary[0],right_boundary[0] :\", left_boundary[0], right_boundary[0])\n",
    "            print(\"left_boundary[1],right_boundary[1] :\", left_boundary[1], right_boundary[1])\n",
    "            print(\"xmin, xmax :\", xmin, xmax)\n",
    "            print(\"ymin, ymax :\", ymin, ymax)\n",
    "            if scores.max() > 0.78:\n",
    "                engine.say(\"Go on\")\n",
    "            if(xmin >= left_boundary[0]):\n",
    "                engine.say(\"move left\")\n",
    "                cv2.putText(frame,'Move LEFT!', (300, 100),cv2.FONT_HERSHEY_SIMPLEX,1.5,(0,255,0),2)\n",
    "            elif(xmax <= right_boundary[0]):\n",
    "                engine.say(\"move right\")\n",
    "                cv2.putText(frame,'Move RIGHT!', (300, 100),cv2.FONT_HERSHEY_SIMPLEX,1.5,(0,255,0),2)\n",
    "            elif(xmin <= left_boundary[0] and xmax >= right_boundary[0]):\n",
    "                engine.say(\"stop\")\n",
    "                cv2.putText(frame,' STOPPPPPP!!!', (300, 100),cv2.FONT_HERSHEY_SIMPLEX,1.5,(0,255,0),2)\n",
    "\n",
    "\n",
    "            cv2.line(frame,tuple(left_boundary),tuple(left_boundary_top), (255, 0, 0), 5)\n",
    "            cv2.line(frame,tuple(right_boundary),tuple(right_boundary_top), (255, 0, 0), 5)\n",
    "           # out.write(frame)\n",
    "            engine.runAndWait()\n",
    "\n",
    "            cv2.imshow('frame',frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def voice(a):\n",
    "        engine.say(a)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def running():\n",
    "#     arr=[detection,voice]\n",
    "#     for i,j in enumerate(arr):\n",
    "#         i=multiprocessing.Process(target=j)\n",
    "#         i.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tim():\n",
    "    t1=threading.Thread(target=detection)\n",
    "    #t2=threading.Thread(target=voice)\n",
    "    t1.start()\n",
    "    #t2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dscs\n",
      "(1920, 1080, 3)\n",
      "600 740\n",
      "0.36117536\n",
      "left_boundary[0],right_boundary[0] : 296 444\n",
      "left_boundary[1],right_boundary[1] : 570 570\n",
      "xmin, xmax : 113 635\n",
      "ymin, ymax : 105 585\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "tim()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b7fa4657efae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 't1' is not defined"
     ]
    }
   ],
   "source": [
    "t1.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
